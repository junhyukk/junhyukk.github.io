<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">    
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.8.2/css/all.min.css">
    <link rel="stylesheet" href="css/junhyuk.css">
    <title>Jun-Hyuk Kim</title>
  </head>

  <body data-spy="scroll" data-target="#my_navbar" data-offset="90">

    <!-- Navigaton bar -->
    <nav class="shadow-sm navbar navbar-expand-lg navbar-light bg-light fixed-top" id="my_navbar">
      <div class="container">
        <!-- Navigaton brand -->
        <a class="navbar-brand font-weight-bold" href="#">Jun-Hyuk Kim</a>
        <a class="navbar-brand" href="http://scholar.google.com/citations?user=A0io6mQAAAAJ&hl=en" target="_blank"><i class="fas fa-graduation-cap"></i></a>
        <a class="navbar-brand" href="http://github.com/junhyukk/" target="_blank"><i class="fab fa-github"></i></a>
        <a class="navbar-brand mr-auto" href="CV.pdf" target="_blank"><i class="fas fa-file-pdf"></i></a>

        <!-- Navtigation toggler button-->
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#my_navbar_contents" aria-controls="my_navbar_contents" aria-expanded="false" aria-label="Toggle navigation">
          <span ><i class="fas fa-bars"></i></span>
        </button>

        <!-- Navtigation toggler contents -->
        <div class="collapse navbar-collapse" id="my_navbar_contents">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item" data-toggle="collapse" data-target=".navbar-collapse.show">
              <a class="nav-link" href="#home"><strong>Home</strong></a>
            </li>
            <li class="nav-item" data-toggle="collapse" data-target=".navbar-collapse.show">
              <a class="nav-link" href="#publications"><strong>Publications</strong></a>
            </li>
            <li class="nav-item" data-toggle="collapse" data-target=".navbar-collapse.show">
              <a class="nav-link" href="#awards"><strong>Awards</strong></a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Section: home -->
    <section class="container section" id="home">
      <div class="row align-items-center">
        <div class="col-md-4 text-center">
          <img src="junhyuk.jpeg" class="rounded-circle w-75">
          <div class="pt-1 pb-3">
            <h3>Jun-Hyuk Kim</h3>
            <h4 class="lead font-weight-normal">Research Scientist</h4>
            <h4 class="lead font-weight-light"><small>junhyuk.kim@yonsei.ac.kr</small></h4>
          </div>
        </div>
        <div class="col-md-8">
          <p class="lead font-weight-normal">
            I am a research scientist at <a href="https://www.sait.samsung.co.kr/saithome/main/main.do" target="_blank">Samsung Advanced Institute of Technology (SAIT)</a>. I received my Ph.D. degree from <a href="https://www.yonsei.ac.kr/en_sc/" target="_blank">Yonsei University</a>, South Korea, in 2022, under the supervision of <a href="https://mcml.yonsei.ac.kr/professor" target="_blank">Prof. Jong-Seok Lee</a>. I am interested in making visual data more valuable in various aspects based on deep learning. In particular, I have focused on developing learned image restoration and compression models for better visual quality and less capacity, respectively.<br /><br />
            <strong>Experiences</strong>
            <li>Research scientist at <a href="https://www.sait.samsung.co.kr/saithome/main/main.do" target="_blank">SAIT</a> (Oct. 2022 - Now)</li>
            <li>Research intern at <a href="https://naver-career.gitbook.io/en/teams/clova-cic" target="_blank">NAVER AI Lab</a> (Jun. 2021 - Nov. 2021)</li>
          </p>    
        </div>
      </div>
      <hr width="100%" align="left" color="#EAECEE">
    </section>

    <!-- Section: News -->
    <section class="container section" id="news">
      <h2><i class="fas fa-bullhorn"></i> News</h2>
      <section class="sub-section">   
      <table class="table-condensed">
        <tbody>
          <tr>
            <td class="news-header">May 2024</td>
            <td>1 paper accepted in ICML'24</td>
          </tr>
          <tr>
            <td class="news-header">Nov. 2022</td>
            <td>1 paper accepted in AAAI'23 (oral presentation)</td>
          </tr>
          <tr>
            <td class="news-header">Jul. 2022</td>
            <td>1 paper accepted in Neurocomputing</td>
          </tr>
          <tr>
            <td class="news-header">Mar. 2022</td>
            <td>1 paper accepted in CVPR'22</td>
          </tr>
          <tr>
            <td class="news-header">Jul. 2021</td>
            <td>1 paper accepted in ICCV'21</td>
          </tr>
          <tr>
            <td class="news-header">Sep. 2020</td>
            <td>1 paper accepted in ACCV'20</td>
          </tr>
          <tr>
            <td class="news-header">Jul. 2020</td>
            <td>1 paper accepted in MM'20 (oral presentation)</td>
          </tr>
          <tr>
            <td class="news-header">Mar. 2020</td>
            <td>1 paper accepted in Neurocomputing</td>
          </tr>
          <tr>
            <td class="news-header">Jan. 2020</td>
            <td>2 papers accepted in ICASSP'20 (1 oral presentation)</td>
          </tr>
        </tbody>
      </table>
      </section>
    </section>

    <!-- Section: publications -->
    <section class="container section" id="publications">
      <h2><i class="fas fa-newspaper"></i> Publications</h2>
      
      <!-- Sub-section: preprints -->
<!--       <section class="sub-section">      
        <h3><li><span>Preprints</span></li></h3>
        <p class="publication-item">
          <span class="lead">Lightweight and efficient image super-resolution with block state-based recursive network</span><br />
          J.-H. Choi, <strong>J.-H. Kim</strong>, M. Cheon, J.-S. Lee<br />
          <em>arXiv:1811.12546</em>, Nov. 2018<br />
          <a href="https://arxiv.org/abs/1811.12546" target="_blank"><i class="fas fa-external-link-square-alt"></i> arXiv</a> / <a href="https://github.com/idearibosome/tf-bsrn-sr" target="_blank"><i class="fab fa-github"></i> GitHub</a>
        </p>
      </section> -->

      
      <!-- Sub-section: conferences -->   
      <section class="sub-section">      
        <h3><li><span>Conferences</span></li></h3>
        <p class="publication-item">
          <span class="lead">Neural image compression with text-guided encoding for both pixel-level and perceptual fidelity</span><br />
          H. Lee, M. Kim, <strong>J.-H. Kim</strong>, S. Kim, D. Oh, J. Lee<br />
          <em>International Conference on Machine Learning (ICML)</em>, Jul. 2024<br />
          <a href="https://arxiv.org/abs/2403.02944" target="_blank"><i class="fas fa-external-link-square-alt"></i> arXiv</a>
        </p>
        <p class="publication-item">
          <span class="lead">Demystifying randomly initialized networks for evaluating generative models</span><br />
          J. Lee, <strong>J.-H. Kim</strong>, J.-S. Lee<br />
          <em>AAAI Conference on Artificial Intelligience (AAAI)</em>, Feb. 2023<br />
          <a href="https://arxiv.org/abs/2208.09218" target="_blank"><i class="fas fa-external-link-square-alt"></i> arXiv</a>
        </p>
        <p class="publication-item">
          <span class="lead">Deep image destruction: Vulnerability of deep image-to-image models against adversarial attacks</span><br />
          J.-H. Choi, H. Zhang, <strong>J.-H. Kim</strong>, C.-J. Hsieh, J.-S. Lee<br />
          <em>International Conference on Pattern Recognition (ICPR)</em>, Aug. 2022<br />
          <a href="https://arxiv.org/abs/2104.15022" target="_blank"><i class="fas fa-external-link-square-alt"></i> arXiv</a>
        </p>
        <p class="publication-item">
          <span class="lead">Joint global and local hierarchical priors for learned image compression</span><br />
          <strong>J.-H. Kim</strong>, B. Heo, J.-S. Lee<br />
          <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pp. 5992-6001, Jun. 2022<br />
          <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kim_Joint_Global_and_Local_Hierarchical_Priors_for_Learned_Image_Compression_CVPR_2022_paper.html" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a> /
          <a href="https://arxiv.org/abs/2112.04487" target="_blank"><i class="fas fa-external-link-square-alt"></i> arXiv</a>
        </p>
        <p class="publication-item">
          <span class="lead">Just one moment: Structural vulnerability of deep action recognition against one frame attack</span><br />
          J. Hwang, <strong>J.-H. Kim</strong>, J.-H. Choi, J.-S. Lee<br />
          <em>IEEE International Conference on Computer Vision (ICCV)</em>, pp. 7668-7676, Oct. 2021<br />
          <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Hwang_Just_One_Moment_Structural_Vulnerability_of_Deep_Action_Recognition_Against_ICCV_2021_paper.html" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a> /
          <a href="https://arxiv.org/abs/2011.14585" target="_blank"><i class="fas fa-external-link-square-alt"></i> arXiv</a>
        </p>
        <p class="publication-item">
          <span class="lead">Edge attention network for image deblurring and super-resolution</span><br />
          J.-W. Han, J.-H. Choi, <strong>J.-H. Kim</strong>, J.-S. Lee<br />
          <em>IEEE International Conference on Systems, Man, and Cybernetics (SMC)</em>, pp. 2393-2398, Oct. 2021<br />
          <a href="https://doi.org/10.1109/SMC52423.2021.9658863" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a>
        </p>
        <p class="publication-item">
          <span class="lead">NTIRE 2021 challenge on image deblurring</span><br />
          S. Nah et al. (including <strong>J.-H. Kim</strong>)<br />
          <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em>, Jun. 2021<br />
          <a href="https://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Nah_NTIRE_2021_Challenge_on_Image_Deblurring_CVPRW_2021_paper.html" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a> /
          <a href="https://arxiv.org/abs/2104.14854" target="_blank"><i class="fas fa-external-link-square-alt"></i> arXiv</a>
        </p>
        <p class="publication-item">
          <span class="lead">Adversarially robust deep image super-resolution using entropy regularization</span><br />
          J.-H. Choi, H. Zhang, <strong>J.-H. Kim</strong>, C.-J. Hsieh, J.-S. Lee<br />
          <em>Asian Conference on Computer Vision (ACCV)</em>, Nov. 2020<br />
          <a href="https://openaccess.thecvf.com/content/ACCV2020/html/Choi_Adversarially_Robust_Deep_Image_Super-Resolution_using_Entropy_Regularization_ACCV_2020_paper.html" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a>
        </p>
        <p class="publication-item">
          <span class="lead">Multi-scale adaptive residual network using total variation for real image super-resolution</span><br />
          K.-H. Ahn, <strong>J.-H. Kim</strong>, J.-H. Choi, J.-S. Lee<br />
          <em>International Conference on Consumer Electronics Asia (ICCE-Asia)</em>, pp. 349-352, Nov. 2020<br />
          <a href="https://doi.org/10.1109/ICCE-Asia49877.2020.9276925" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a>
        </p>
        <p class="publication-item">
          <span class="lead">Efficient bokeh effect rendering using generative adversarial network</span><br />
          M.-S. Choi, <strong>J.-H. Kim</strong>, J.-H. Choi, J.-S. Lee<br />
          <em>International Conference on Consumer Electronics Asia (ICCE-Asia)</em>, pp. 404-408, Nov. 2020<br />
          <a href="https://doi.org/10.1109/ICCE-Asia49877.2020.9276807" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a>
        </p>
        <p class="publication-item">
          <span class="lead">Instability of successive deep image compression</span><br />
          <strong>J.-H. Kim</strong>, S. Jang, J.-H. Choi, J.-S. Lee<br />
          <em>ACM Multimedia (MM)</em>, pp. 247-255, Oct. 2020<br />
          <a href="https://doi.org/10.1145/3394171.3413680" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a>
        </p>
        <p class="publication-item">
          <span class="lead">LarvaNet: Hierarchical super-resolution via multi-exit architecture</span><br />
          G.-W. Jeon, J.-H. Choi, <strong>J.-H. Kim</strong>, J.-S. Lee<br />
          <em>European Conference on Computer Vision (ECCV) Workshops</em>, Aug. 2020<br />
          <a href="https://doi.org/10.1007/978-3-030-67070-2_4" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a>
        </p>
        <p class="publication-item">
          <span class="lead">AIM 2020 challenge on efficient super-resolution: methods and results</span><br />
          K. Zhang et al. (including <strong>J.-H. Kim</strong>)<br />
          <em>European Conference on Computer Vision (ECCV) Workshops</em>, Aug. 2020<br />
          <a href="https://arxiv.org/abs/2009.06943" target="_blank"><i class="fas fa-external-link-square-alt"></i> arXiv</a>
        </p>
        <p class="publication-item">
          <span class="lead">AIM 2020 challenge on real image super-resolution: methods and results</span><br />
          P. Wei et al. (including <strong>J.-H. Kim</strong>)<br />
          <em>European Conference on Computer Vision (ECCV) Workshops</em>, Aug. 2020<br />
          <a href="https://arxiv.org/abs/2009.12072" target="_blank"><i class="fas fa-external-link-square-alt"></i> arXiv</a>
        </p>
        <p class="publication-item">
          <span class="lead">Efficient deep learning-based lossy image compression via asymmetric autoencoder and pruning</span><br />
          <strong>J.-H. Kim</strong>, J.-H. Choi, J. Chang, J.-S. Lee<br />
          <em>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp. 2063-2067, May 2020<br />
          <a href="https://doi.org/10.1109/ICASSP40776.2020.9053102" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a>
        </p>
        <p class="publication-item">
          <span class="lead">SRZoo: An integrated repository for super-resolution using deep learning</span><br />
          J.-H. Choi, <strong>J.-H. Kim</strong>, J.-S. Lee<br />
          <em>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp. 2508-2512, May 2020<br />
          <a href="https://doi.org/10.1109/ICASSP40776.2020.9054533" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a> /
          <a href="https://arxiv.org/abs/2006.01339" target="_blank"><i class="fas fa-external-link-square-alt"></i> arXiv</a> /
          <a href="https://github.com/idearibosome/srzoo" target="_blank"><i class="fab fa-github"></i> GitHub</a>
        </p>
        <p class="publication-item">
          <span class="lead">Evaluating robustness of deep image super-resolution against adversarial attacks</span><br />
          J.-H. Choi, H. Zhang, <strong>J.-H. Kim</strong>, C.-J. Hsieh, J.-S. Lee<br />
          <em>IEEE International Conference on Computer Vision (ICCV)</em>, pp. 303-311, Oct. 2019<br />
          <a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Choi_Evaluating_Robustness_of_Deep_Image_Super-Resolution_Against_Adversarial_Attacks_ICCV_2019_paper.html" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a> / <a href="https://arxiv.org/abs/1904.06097" target="_blank"><i class="fas fa-external-link-square-alt"></i> arXiv</a>
        </p>
        <p class="publication-item">
          <span class="lead">Deep learning-based super-resolution for digital comics</span><br />
          <strong>J.-H. Kim</strong>, J.-H. Choi, C.-H. Seo, J. Chang, J.-S. Lee<br />
          <em>SIGGRAPH Asia 2018 Posters</em>, Article 19, Dec. 2018<br />
          <a href="https://doi.org/10.1145/3283289.3283337" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a>
        </p>
        <p class="publication-item">
          <span class="lead">Generative adversarial network-based image super-resolution using perceptual content losses</span><br />
          M. Cheon, <strong>J.-H. Kim</strong>, J.-H. Choi, J.-S. Lee<br />
          <em>European Conference on Computer Vision (ECCV) Workshops</em>, pp. 51-62, Sep. 2018<br />
          <a href="https://doi.org/10.1007/978-3-030-11021-5_4" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a> / <a href="http://arxiv.org/abs/1809.04783" target="_blank"><i class="fas fa-external-link-square-alt"></i> arXiv</a> / <a href="https://github.com/manricheon/eusr-pcl-tf" target="_blank"><i class="fab fa-github"></i> GitHub</a>
        </p>
        <p class="publication-item">
          <span class="lead">Deep residual network with enhanced upscaling module for super-resolution</span><br />
          <strong>J.-H. Kim</strong>, J.-S. Lee<br />
          <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em>, pp. 913-921, Jun. 2018<br />
          <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w13/html/Kim_Deep_Residual_Network_CVPR_2018_paper.html" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a> / <a href="https://github.com/junhyukk/EUSR-Tensorflow" target="_blank"><i class="fab fa-github"></i> GitHub</a>
        </p>
        <p class="publication-item">
          <span class="lead">NTIRE 2018 challenge on single image super-resolution: methods and results</span><br />
          R. Timofte et al. (including <strong>J.-H. Kim</strong>)<br />
          <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em>, pp. 965-976, Jun. 2018<br />
          <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w13/html/Timofte_NTIRE_2018_Challenge_CVPR_2018_paper.html" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a>
        </p>
        <p class="publication-item">
          <span class="lead">Assessing product design using photos and real products</span><br />
          S.-E. Moon, <strong>J.-H. Kim</strong>, S.-W. Kim, J.-S. Lee<br />
          <em>ACM Conf. Human Factors in Computing Systems (CHI), Extended Abstract</em>, pp. 1100-1107, May 2017<br />
          <a href="https://doi.org/10.1145/3027063.3053336" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a>
        </p>
        <p class="publication-item">
          <span class="lead">Travel photo album summarization based on aesthetic quality, interestingness, and memorableness</span><br />
          <strong>J.-H. Kim</strong>, J.-S. Lee<br />
          <em>APSIPA Annual Summit and Conference</em>, Dec. 2016<br />
          <a href="https://doi.org/10.1109/APSIPA.2016.7820889" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a>
        </p>
      </section>

      <!-- Sub-section: journals -->
      <section class="sub-section">      
        <h3><li><span>Journals</span></li></h3>
        <p class="publication-item">
          <span class="lead">Successive learned image compression: Comprehensive analysis of instability</span><br />
          <strong>J.-H. Kim</strong>, S. Jang, J.-H. Choi, J.-S. Lee<br />
          <em>Neurocomputing</em>, vol. 506, pp. 12-24, Sep. 2022<br />
          <a href="https://doi.org/10.1016/j.neucom.2022.07.065" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a> 
        </p>
        <p class="publication-item">
          <span class="lead">Volatile-nonvolatile memory network for progressive image super-resolution</span><br />
          J.-H. Choi, <strong>J.-H. Kim</strong>, M. Cheon, J.-S. Lee<br />
          <em>IEEE Access</em>, vol. 9, pp. 37487-37496, Mar. 2021<br />
          <a href="https://doi.org/10.1109/ACCESS.2021.3063760" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a> / <a href="https://github.com/idearibosome/tf-vmnet" target="_blank"><i class="fab fa-github"></i> GitHub</a>
        </p>
        <p class="publication-item">
          <span class="lead">Prediction of car design perception using EEG and gaze patterns</span><br />
          S.-E. Moon, <strong>J.-H. Kim</strong>, S.-W. Kim, J.-S. Lee<br />
          <em>IEEE Transactions on Affective Computing</em>, vol. 12, no. 4, pp. 843-856, 2021<br />
          <a href="https://doi.org/10.1109/TAFFC.2019.2901733" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a>
        </p>  
        <p class="publication-item">
          <span class="lead">MAMNet: Multi-path adaptive modulation network for image super-resolution</span><br />
          <strong>J.-H. Kim</strong>, J.-H. Choi, M. Cheon, J.-S. Lee<br />
          <em>Neurocomputing</em>, vol. 402, pp. 38-49, Aug. 2020<br />
          <a href="https://doi.org/10.1016/j.neucom.2020.03.069" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a> / <a href="https://arxiv.org/abs/1811.12043" target="_blank"><i class="fas fa-external-link-square-alt"></i> arXiv</a> / <a href="https://github.com/junhyukk/MAMNet-tensorflow" target="_blank"><i class="fab fa-github"></i> GitHub</a>
        </p>
        <p class="publication-item">
          <span class="lead">Deep learning-based image super-resolution considering quantitative and perceptual quality</span><br />
          J.-H. Choi, <strong>J.-H. Kim</strong>, M. Cheon, J.-S. Lee<br />
          <em>Neurocomputing</em>, vol. 398, pp. 347-359, Jul. 2020<br />
          <a href="https://doi.org/10.1016/j.neucom.2019.06.103" target="_blank"><i class="fas fa-external-link-square-alt"></i> Detail</a> / 
          <a href="https://arxiv.org/abs/1809.04789" target="_blank"><i class="fas fa-external-link-square-alt"></i> arXiv</a> / <a href="https://github.com/idearibosome/tf-perceptual-eusr" target="_blank"><i class="fab fa-github"></i> GitHub</a>
        </p>    
      </section>
    </section>

    <!-- Section: awards -->
    <section class="container section" id="awards">
      <h2><i class="fas fa-award"></i> Awards</h2>
      <section class="sub-section">   
        <p class="award-item">
          <span class="lead">Merit Academic Paper Award</span><br />
          <em>2020-2 Yonsei Superior Paper Awards</em>, Dec. 2020<br />
          <!-- <small>Korean: 2020년 2학기 연세대학교 대학원생 우수논문 장려상</small> -->
        </p>   
        <p class="award-item">
          <span class="lead">2nd Place (Region 1)</span><br />
          Yonsei-MCML team<br />
          <em>Super-Resolution Challenge on Perceptual Image Restoration and Manipulation (PIRM)</em>, in conjunction with ECCV, Sep. 2018<br />
          <a href="https://www.pirm2018.org/PIRM-SR.html#leaderboard" target="_blank"><i class="fas fa-external-link-square-alt"></i> Leaderboard</a> / <a href="https://github.com/manricheon/eusr-pcl-tf" target="_blank"><i class="fab fa-github"></i> GitHub</a>
        </p>
        <p class="award-item">
          <span class="lead">2nd Place (Region 2)</span><br />
          Yonsei-MCML team<br />
          <em>Super-Resolution Challenge on Perceptual Image Restoration and Manipulation (PIRM)</em>, in conjunction with ECCV, Sep. 2018<br />
          <a href="https://www.pirm2018.org/PIRM-SR.html#leaderboard" target="_blank"><i class="fas fa-external-link-square-alt"></i> Leaderboard</a> / <a href="https://github.com/idearibosome/tf-perceptual-eusr" target="_blank"><i class="fab fa-github"></i> GitHub</a>
        </p>
      </section>
    </section>

    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>

  </body>
</html>
